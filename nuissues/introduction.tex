\section{Introduction}

\citeauthor{norman:natural-user-interfaces-are-not-natural:2010}'s (\citeyear{norman:natural-user-interfaces-are-not-natural:2010}) opinion piece \citetitle{norman:natural-user-interfaces-are-not-natural:2010} begins with the following quote from Steve Ballmer, CEO of Microsoft:

\begin{quote}
I believe we will look back on 2010 as the year we expanded beyond the mouse and keyboard, and started incorporating more natural forms of interaction such as touch, speech, gestures, handwriting, and vision -- what computer scientists call the 'NUI' or natural user interface."
\end{quote}


Following the quotation, a statement is made:

\begin{quotation}
[\dots] A new world of interaction is here: The rulebooks and guidelines are being rewritten, or at least, such is the claim. And the new interactions even have a new marketing name: natural, as in "Natural User Interface."

As usual, marketing rhetoric is ahead of reality.    
\end{quotation}

\citeauthor{norman:natural-user-interfaces-are-not-natural:2010} is, of course, right: using Microsoft as an example, we have seen attempts at natural user interfaces in the direction of hybrid tablet computers like the Microsoft Surface, gesture trackers like Microsoft Kinect, and voice recognition in the personal assistant Cortana. Though efforts have certainly been made, we have mostly seen that Natural User Interfaces (NUIs) are just a new way to interact with the old (but sometimes slightly different-looking) interfaces we already know. A prime example how this application does not work without thought put into optimising for the task at hand is the "flat" look of Windows called "Metro" and then "Modern UI", which was supposed to fit \textit{all} Microsoft devices (desktops, laptops, tablets, and smartphones).

Natural User Interfaces are, indeed, a new way to interact with Graphical User Interfaces (GUIs), and not something completely new. However, as we move away from pen and mouse and into the domain of touch sensitivity, speech recognition, and several other "natural" interface types, we must also update the GUIs to suit the task at hand and the input type of choice.

For example, a tablet with a screen surface sensitive to both the natural interface of touch and the Tangible User Interface (TUI) of stylus (pen) should immediately be able to differentiate between the two input types, because a user normally only wants to use the stylus for tasks fine-grained than what can be done just as efficiently by touching the screen with fingers: for example, sketching or annotating a document.

When it has been established that NUIs are new ways of interacting with GUIs and thus get all of the benefits (and restrictions) of the GUI's exploratory nature, we must of course learn from the GUI design and how we have interacted with them before to best design natural interaction experiences -- but looking to the past is far from enough. New guidelines must be written and revised before we can determine a common understanding of how natural user interfaces should be used. After all, \textcite{norman:natural-user-interfaces-are-not-natural:2010} calls them \textit{not natural}.

Typically, NUIs respond to everyday, natural interactions such as touch and speech, instead of forcing the user into using a Tangible User Interface (TUI).

\begin{itemize}
  \item Technical challenge: recognise different forms of input (e.g., pen vs. mouse vs. touch, multitouch) and prompt/respond differently/properly to type of input (especially separating between different TUIs like mouse and pen -- you only want to sketch/annotate with a pen). See Photoshop as example for this (pressure recognition, etc.)
\end{itemize}

Have designed a very commonly used system: an issue (task) tracker. Focus has been on optimising the actual board interaction for touch, as this is the natural interface type that should be expected to best optimise the key flows in the application (moving tasks back and forth between swimlanes as the status of the tasks changes).


How can a 2D interface optimised for the natural interface type \textit{touch} help boost productivity in a simple issue tracking system?


with focus on natural (touch) interface which should optimise the key flows in the application: a task tracker.

\begin{itemize}
  \item NUI types: only touch (skin is unrelated to the domain, but gestures (e.g., with a kinect in a meeting room) is interesting, speech is unrelated unless the audience has no arms/ability to touch (disabled), gaze tracking is cool but unnecessary unless same as speech recognition, brain machine interface only relevant for heavily disabled users who are probably not in need of an issue tracker)
\end{itemize}
